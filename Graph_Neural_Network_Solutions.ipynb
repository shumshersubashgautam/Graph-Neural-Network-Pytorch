{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHsBKt9l3yK1lgSZ/QUdzF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shumshersubashgautam/Graph-Neural-Network-Pytorch/blob/main/Graph_Neural_Network_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLMiDL6kHRCF",
        "outputId": "5f1d34c0-f6dd-4acf-a9ed-e5932d253af7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-scatter\n",
            "  Using cached torch_scatter-2.1.1.tar.gz (107 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-sparse\n",
            "  Using cached torch_sparse-0.6.17.tar.gz (209 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-geometric\n",
            "  Using cached torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-scatter, torch-sparse, torch-geometric\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install torch-scatter torch-sparse torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH3Y3lv0MVA0",
        "outputId": "646e6f83-10bf-460b-9fb5-76bb2556f516"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.17%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.17+pt20cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.22.4)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.1+pt20cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (884 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m884.9/884.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt20cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Using cached torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=4e8271dc0fbbaddd557c742eed6384e816c22ea7471c6a34558cd7b842a3d4f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pytorch Geometric Framework**"
      ],
      "metadata": {
        "id": "ViuV90b3HtLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "edge_index = torch.tensor([[2, 1, 3],\n",
        "                           [0, 0, 2]], dtype=torch.long)\n",
        "x = torch.tensor([[1], [1], [1]], dtype=torch.float)\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07vPOPXEHfYR",
        "outputId": "77e8ad68-15da-4876-fdea-c52f844cc19f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[3, 1], edge_index=[2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mini-Batching Graphs**\n",
        "\n",
        "Implementing the GCN layer\n",
        "Add self-loops to the adjacency matrix.\n",
        "\n",
        "Linearly transform node feature matrix.\n",
        "\n",
        "Normalize node features.\n",
        "\n",
        "Sum up neighboring node features.\n",
        "\n",
        "Return new node embeddings."
      ],
      "metadata": {
        "id": "aal4SPv2Hz3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.nn import MessagePassing\n",
        "import math\n",
        "\n",
        "def glorot(tensor):\n",
        "    if tensor is not None:\n",
        "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
        "        tensor.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "def zeros(tensor):\n",
        "    if tensor is not None:\n",
        "        tensor.data.fill_(0)\n",
        "\n",
        "\n",
        "def add_self_loops(edge_index, num_nodes=None):\n",
        "    loop_index = torch.arange(0, num_nodes, dtype=torch.long,\n",
        "                              device=edge_index.device)\n",
        "    loop_index = loop_index.unsqueeze(0).repeat(2, 1)\n",
        "\n",
        "    edge_index = torch.cat([edge_index, loop_index], dim=1)\n",
        "\n",
        "    return edge_index\n",
        "\n",
        "\n",
        "def degree(index, num_nodes=None, dtype=None):\n",
        "    out = torch.zeros((num_nodes), dtype=dtype, device=index.device)\n",
        "    return out.scatter_add_(0, index, out.new_ones((index.size(0))))\n",
        "\n",
        "\n",
        "class GCNConv(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
        "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        glorot(self.lin.weight)\n",
        "        zeros(self.lin.bias)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x has shape [N, in_channels]\n",
        "        # edge_index has shape [2, E]\n",
        "\n",
        "        ########################################################################\n",
        "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
        "        ########################################################################\n",
        "        # Step 1: Add self-loops to the adjacency matrix.\n",
        "\n",
        "        edge_index = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "\n",
        "        # Step 2: Linearly transform node feature matrix.\n",
        "        x = self.lin(x)\n",
        "\n",
        "        # Step 3-5: Start propagating messages.\n",
        "\n",
        "        return self.propagate(edge_index, x=x)\n",
        "        ########################################################################\n",
        "        #                             END OF YOUR CODE                         #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def message(self, x_j, edge_index, size):\n",
        "        # x_j has shape [E, out_channels]\n",
        "\n",
        "        ########################################################################\n",
        "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
        "        ########################################################################\n",
        "\n",
        "        # Step 3: Normalize node features.\n",
        "        row, col = edge_index\n",
        "        deg = degree(row, size[0], dtype=x_j.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "        return norm.view(-1, 1) * x_j\n",
        "\n",
        "        ########################################################################\n",
        "        #                             END OF YOUR CODE                         #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        # aggr_out has shape [N, out_channels]\n",
        "\n",
        "        # Step 5: Return new node embeddings.\n",
        "        return aggr_out"
      ],
      "metadata": {
        "id": "NMUqx7weH1cC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Abstract Message Passing Scheme in PyG**"
      ],
      "metadata": {
        "id": "tasPOlATIMpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "\n",
        "def uniform(size, tensor):\n",
        "    bound = 1.0 / math.sqrt(size)\n",
        "    if tensor is not None:\n",
        "        tensor.data.uniform_(-bound, bound)\n",
        "\n",
        "\n",
        "class SAGEConv(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, aggr):\n",
        "        super(SAGEConv, self).__init__(aggr=aggr)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        self.weight = Parameter(torch.Tensor(2 * in_channels, out_channels))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        uniform(self.weight.size(0), self.weight)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "\n",
        "        ########################################################################\n",
        "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
        "        ########################################################################\n",
        "\n",
        "        return self.propagate(edge_index, x=x)\n",
        "\n",
        "        ########################################################################\n",
        "        #                             END OF YOUR CODE                         #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "\n",
        "    def message(self, x_j, edge_weight):\n",
        "\n",
        "        ########################################################################\n",
        "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
        "        ########################################################################\n",
        "\n",
        "        return x_j\n",
        "\n",
        "        ########################################################################\n",
        "        #                             END OF YOUR CODE                         #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "\n",
        "    def update(self, aggr_out, x):\n",
        "\n",
        "\n",
        "        ########################################################################\n",
        "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
        "        ########################################################################\n",
        "\n",
        "        aggr_out = torch.cat([x, aggr_out], dim=-1)\n",
        "        aggr_out = torch.matmul(aggr_out, self.weight)\n",
        "        aggr_out = F.normalize(aggr_out, p=2, dim=-1)\n",
        "\n",
        "        return aggr_out\n",
        "\n",
        "        ########################################################################\n",
        "        #                             END OF YOUR CODE                         #\n",
        "        ########################################################################\n",
        ""
      ],
      "metadata": {
        "id": "euFOqsmFIOsL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vertex Classification**"
      ],
      "metadata": {
        "id": "zw2EmxJZStps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os.path as osp\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "path = osp.join(os.getcwd(), 'data', 'Cora')\n",
        "dataset = Planetoid(path, 'Cora')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iHARiVBSqNe",
        "outputId": "b412950a-2d74-4cef-df25-e8085bafece3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "from torch import tensor\n",
        "from torch.optim import Adam\n",
        "\n",
        "def run(dataset, model, runs, epochs, lr, weight_decay, early_stopping):\n",
        "\n",
        "    val_losses, accs, durations = [], [], []\n",
        "    for _ in range(runs):\n",
        "        data = dataset[0]\n",
        "        data = data.to(device)\n",
        "\n",
        "        model.to(device).reset_parameters()\n",
        "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        t_start = time.perf_counter()\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        test_acc = 0\n",
        "        val_loss_history = []\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            train(model, optimizer, data)\n",
        "            eval_info = evaluate(model, data)\n",
        "            eval_info['epoch'] = epoch\n",
        "\n",
        "            if eval_info['val_loss'] < best_val_loss:\n",
        "                best_val_loss = eval_info['val_loss']\n",
        "                test_acc = eval_info['test_acc']\n",
        "\n",
        "            val_loss_history.append(eval_info['val_loss'])\n",
        "            if early_stopping > 0 and epoch > epochs // 2:\n",
        "                tmp = tensor(val_loss_history[-(early_stopping + 1):-1])\n",
        "                if eval_info['val_loss'] > tmp.mean().item():\n",
        "                    break\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        t_end = time.perf_counter()\n",
        "\n",
        "        val_losses.append(best_val_loss)\n",
        "        accs.append(test_acc)\n",
        "        durations.append(t_end - t_start)\n",
        "\n",
        "    loss, acc, duration = tensor(val_losses), tensor(accs), tensor(durations)\n",
        "\n",
        "    print('Val Loss: {:.4f}, Test Accuracy: {:.3f} ± {:.3f}, Duration: {:.3f}'.\n",
        "          format(loss.mean().item(),\n",
        "                 acc.mean().item(),\n",
        "                 acc.std().item(),\n",
        "                 duration.mean().item()))\n",
        "\n",
        "\n",
        "def train(model, optimizer, data):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "def evaluate(model, data):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(data)\n",
        "\n",
        "    outs = {}\n",
        "    for key in ['train', 'val', 'test']:\n",
        "        mask = data['{}_mask'.format(key)]\n",
        "        loss = F.nll_loss(logits[mask], data.y[mask]).item()\n",
        "        pred = logits[mask].max(1)[1]\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "\n",
        "        outs['{}_loss'.format(key)] = loss\n",
        "        outs['{}_acc'.format(key)] = acc\n",
        "\n",
        "    return outs"
      ],
      "metadata": {
        "id": "zDjT9Xx8SxRj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the model with GCN on vertex classification"
      ],
      "metadata": {
        "id": "-jEqCr4FS6Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "runs = 10\n",
        "epochs = 200\n",
        "lr = 0.01\n",
        "weight_decay = 0.0005\n",
        "early_stopping = 10\n",
        "hidden = 16\n",
        "dropout = 0.5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, dataset):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        ########################################################################\n",
        "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
        "        ########################################################################\n",
        "\n",
        "        self.conv1 = GCNConv(dataset.num_features, hidden)\n",
        "        self.conv2 = GCNConv(hidden, dataset.num_classes)\n",
        "\n",
        "        ########################################################################\n",
        "        #                             END OF YOUR CODE                         #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv1.reset_parameters()\n",
        "        self.conv2.reset_parameters()\n",
        "\n",
        "    def forward(self, data):\n",
        "\n",
        "        ########################################################################\n",
        "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
        "        ########################################################################\n",
        "\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "        ########################################################################\n",
        "        #                             END OF YOUR CODE                         #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "run(dataset, Net(dataset), runs, epochs, lr, weight_decay,\n",
        "    early_stopping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hObC3BD4S4dv",
        "outputId": "4cea818b-e82f-451c-ce7b-87a1804fdedf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.7396, Test Accuracy: 0.798 ± 0.009, Duration: 0.900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build models with GraphSAGE on vertex classification (lab)"
      ],
      "metadata": {
        "id": "1crS1_CRTBPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Graph Classification\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.utils import degree\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "\n",
        "class NormalizedDegree(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, data):\n",
        "        deg = degree(data.edge_index[0], dtype=torch.float)\n",
        "        deg = (deg - self.mean) / self.std\n",
        "        data.x = deg.view(-1, 1)\n",
        "        return data\n",
        "\n",
        "\n",
        "def get_dataset(name, cleaned=False):\n",
        "    path = osp.join(os.getcwd(), 'data', name)\n",
        "    dataset = TUDataset(path, name, cleaned=cleaned)\n",
        "    dataset.data.edge_attr = None\n",
        "\n",
        "    if dataset.data.x is None:\n",
        "        max_degree = 0\n",
        "        degs = []\n",
        "        for data in dataset:\n",
        "            degs += [degree(data.edge_index[0], dtype=torch.long)]\n",
        "            max_degree = max(max_degree, degs[-1].max().item())\n",
        "\n",
        "        if max_degree < 1000:\n",
        "            dataset.transform = T.OneHotDegree(max_degree)\n",
        "        else:\n",
        "            deg = torch.cat(degs, dim=0).to(torch.float)\n",
        "            mean, std = deg.mean().item(), deg.std().item()\n",
        "            dataset.transform = NormalizedDegree(mean, std)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "DlkLVpyOS-Wx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_dataset(dataset):\n",
        "    num_nodes = num_edges = 0\n",
        "    for data in dataset:\n",
        "        num_nodes += data.num_nodes\n",
        "        num_edges += data.num_edges\n",
        "\n",
        "    print('Name', dataset)\n",
        "    print('Graphs', len(dataset))\n",
        "    print('Nodes', num_nodes / len(dataset))\n",
        "    print('Edges', (num_edges // 2) / len(dataset))\n",
        "    print('Features', dataset.num_features)\n",
        "    print('Classes', dataset.num_classes)\n",
        "    print()\n",
        "\n",
        "\n",
        "for name in ['IMDB-BINARY']:\n",
        "    print_dataset(get_dataset(name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJjhHoxMTFQp",
        "outputId": "cf95054e-4cdc-42d1-ab5f-94e92e48f996"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/IMDB-BINARY.zip\n",
            "Extracting /content/data/IMDB-BINARY/IMDB-BINARY/IMDB-BINARY.zip\n",
            "Processing...\n",
            "Done!\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name IMDB-BINARY(1000)\n",
            "Graphs 1000\n",
            "Nodes 19.773\n",
            "Edges 96.531\n",
            "Features 136\n",
            "Classes 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import tensor\n",
        "from torch.optim import Adam\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch_geometric.data import DataLoader, DenseDataLoader as DenseLoader\n",
        "\n",
        "\n",
        "def cross_validation_with_val_set(dataset, model, folds, epochs, batch_size,\n",
        "                                  lr, lr_decay_factor, lr_decay_step_size,\n",
        "                                  weight_decay, logger=None):\n",
        "\n",
        "    val_losses, accs, durations = [], [], []\n",
        "    for fold, (train_idx, test_idx,\n",
        "               val_idx) in enumerate(zip(*k_fold(dataset, folds))):\n",
        "\n",
        "        train_dataset = dataset[train_idx]\n",
        "        test_dataset = dataset[test_idx]\n",
        "        val_dataset = dataset[val_idx]\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
        "\n",
        "        model.to(device).reset_parameters()\n",
        "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        t_start = time.perf_counter()\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            train_loss = train(model, optimizer, train_loader)\n",
        "            val_losses.append(eval_loss(model, val_loader))\n",
        "            accs.append(eval_acc(model, test_loader))\n",
        "            eval_info = {\n",
        "                'fold': fold,\n",
        "                'epoch': epoch,\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_losses[-1],\n",
        "                'test_acc': accs[-1],\n",
        "            }\n",
        "\n",
        "            if logger is not None:\n",
        "                logger(eval_info)\n",
        "\n",
        "            if epoch % lr_decay_step_size == 0:\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    param_group['lr'] = lr_decay_factor * param_group['lr']\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        t_end = time.perf_counter()\n",
        "        durations.append(t_end - t_start)\n",
        "\n",
        "    loss, acc, duration = tensor(val_losses), tensor(accs), tensor(durations)\n",
        "    loss, acc = loss.view(folds, epochs), acc.view(folds, epochs)\n",
        "    loss, argmin = loss.min(dim=1)\n",
        "    acc = acc[torch.arange(folds, dtype=torch.long), argmin]\n",
        "\n",
        "    loss_mean = loss.mean().item()\n",
        "    acc_mean = acc.mean().item()\n",
        "    acc_std = acc.std().item()\n",
        "    duration_mean = duration.mean().item()\n",
        "    print('Val Loss: {:.4f}, Test Accuracy: {:.3f} ± {:.3f}, Duration: {:.3f}'.\n",
        "          format(loss_mean, acc_mean, acc_std, duration_mean))\n",
        "\n",
        "    return loss_mean, acc_mean, acc_std\n",
        "\n",
        "\n",
        "def k_fold(dataset, folds):\n",
        "    skf = StratifiedKFold(folds, shuffle=True, random_state=12345)\n",
        "\n",
        "    test_indices, train_indices = [], []\n",
        "    for _, idx in skf.split(torch.zeros(len(dataset)), dataset.data.y):\n",
        "        test_indices.append(torch.from_numpy(idx))\n",
        "\n",
        "    val_indices = [test_indices[i - 1] for i in range(folds)]\n",
        "\n",
        "    for i in range(folds):\n",
        "        train_mask = torch.ones(len(dataset), dtype=torch.bool)\n",
        "        train_mask[test_indices[i]] = 0\n",
        "        train_mask[val_indices[i]] = 0\n",
        "        train_indices.append(train_mask.nonzero().view(-1))\n",
        "\n",
        "    return train_indices, test_indices, val_indices\n",
        "\n",
        "\n",
        "def num_graphs(data):\n",
        "    if data.batch is not None:\n",
        "        return data.num_graphs\n",
        "    else:\n",
        "        return data.x.size(0)\n",
        "\n",
        "\n",
        "def train(model, optimizer, loader):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = data.to(device)\n",
        "        out = model(data)\n",
        "        loss = F.nll_loss(out, data.y.view(-1))\n",
        "        loss.backward()\n",
        "        total_loss += loss.item() * num_graphs(data)\n",
        "        optimizer.step()\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "\n",
        "def eval_acc(model, loader):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        with torch.no_grad():\n",
        "            pred = model(data).max(1)[1]\n",
        "        correct += pred.eq(data.y.view(-1)).sum().item()\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "\n",
        "def eval_loss(model, loader):\n",
        "    model.eval()\n",
        "\n",
        "    loss = 0\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        with torch.no_grad():\n",
        "            out = model(data)\n",
        "        loss += F.nll_loss(out, data.y.view(-1), reduction='sum').item()\n",
        "    return loss / len(loader.dataset)"
      ],
      "metadata": {
        "id": "AFmyk5fsTcCr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement Graph Isomorphism Network (lab)"
      ],
      "metadata": {
        "id": "Lar-2CWTTiGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement models with GIN-0/GIN-\n",
        " to perform graph classificaiton on IMDB-binary dataset (lab)"
      ],
      "metadata": {
        "id": "i6_q0mpJTl4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d as BN\n",
        "from torch_geometric.nn import global_mean_pool, MessagePassing\n",
        "from torch_geometric.utils import remove_self_loops\n",
        "\n",
        "def reset(nn):\n",
        "    def _reset(item):\n",
        "        if hasattr(item, 'reset_parameters'):\n",
        "            item.reset_parameters()\n",
        "\n",
        "    if nn is not None:\n",
        "        if hasattr(nn, 'children') and len(list(nn.children())) > 0:\n",
        "            for item in nn.children():\n",
        "                _reset(item)\n",
        "        else:\n",
        "            _reset(nn)\n",
        "\n",
        "\n",
        "class GINConv(MessagePassing):\n",
        "    def __init__(self, nn, eps=0, train_eps=False, **kwargs):\n",
        "        super(GINConv, self).__init__(aggr='add', **kwargs)\n",
        "        self.nn = nn\n",
        "        self.initial_eps = eps\n",
        "        if train_eps:\n",
        "            self.eps = torch.nn.Parameter(torch.Tensor([eps]))\n",
        "        else:\n",
        "            self.register_buffer('eps', torch.Tensor([eps]))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        reset(self.nn)\n",
        "        self.eps.data.fill_(self.initial_eps)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        \"\"\"\"\"\"\n",
        "\n",
        "        ########################################################################\n",
        "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
        "        ########################################################################\n",
        "\n",
        "        edge_index, _ = remove_self_loops(edge_index)\n",
        "        out = self.nn((1 + self.eps) * x + self.propagate(edge_index, x=x))\n",
        "        return out\n",
        "\n",
        "        ########################################################################\n",
        "        #                             END OF YOUR CODE                         #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "\n",
        "    def message(self, x_j):\n",
        "\n",
        "        ########################################################################\n",
        "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
        "        ########################################################################\n",
        "\n",
        "        return x_j\n",
        "\n",
        "        ########################################################################\n",
        "        #                             END OF YOUR CODE                         #\n",
        "        ########################################################################"
      ],
      "metadata": {
        "id": "XdQBUAmCTgH2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GIN0(torch.nn.Module):\n",
        "    def __init__(self, dataset, num_layers, hidden):\n",
        "        super(GIN0, self).__init__()\n",
        "        self.conv1 = GINConv(Sequential(\n",
        "            Linear(dataset.num_features, hidden),\n",
        "            ReLU(),\n",
        "            Linear(hidden, hidden),\n",
        "            ReLU(),\n",
        "            BN(hidden),\n",
        "        ),\n",
        "                             train_eps=False)\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for i in range(num_layers - 1):\n",
        "            self.convs.append(\n",
        "                GINConv(Sequential(\n",
        "                    Linear(hidden, hidden),\n",
        "                    ReLU(),\n",
        "                    Linear(hidden, hidden),\n",
        "                    ReLU(),\n",
        "                    BN(hidden),\n",
        "                ),\n",
        "                        train_eps=False))\n",
        "        self.lin1 = Linear(hidden, hidden)\n",
        "        self.lin2 = Linear(hidden, dataset.num_classes)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv1.reset_parameters()\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        self.lin1.reset_parameters()\n",
        "        self.lin2.reset_parameters()\n",
        "\n",
        "    def forward(self, data):\n",
        "\n",
        "        ########################################################################\n",
        "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
        "        ########################################################################\n",
        "\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "        ########################################################################\n",
        "        #                             END OF YOUR CODE                         #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GIN(torch.nn.Module):\n",
        "    def __init__(self, dataset, num_layers, hidden):\n",
        "        super(GIN, self).__init__()\n",
        "        self.conv1 = GINConv(Sequential(\n",
        "            Linear(dataset.num_features, hidden),\n",
        "            ReLU(),\n",
        "            Linear(hidden, hidden),\n",
        "            ReLU(),\n",
        "            BN(hidden),\n",
        "        ),\n",
        "                             train_eps=True)\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for i in range(num_layers - 1):\n",
        "            self.convs.append(\n",
        "                GINConv(Sequential(\n",
        "                    Linear(hidden, hidden),\n",
        "                    ReLU(),\n",
        "                    Linear(hidden, hidden),\n",
        "                    ReLU(),\n",
        "                    BN(hidden),\n",
        "                ),\n",
        "                        train_eps=True))\n",
        "        self.lin1 = Linear(hidden, hidden)\n",
        "        self.lin2 = Linear(hidden, dataset.num_classes)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv1.reset_parameters()\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        self.lin1.reset_parameters()\n",
        "        self.lin2.reset_parameters()\n",
        "\n",
        "    def forward(self, data):\n",
        "\n",
        "        ########################################################################\n",
        "        #      START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)             #\n",
        "        ########################################################################\n",
        "\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "\n",
        "        ########################################################################\n",
        "        #                             END OF YOUR CODE                         #\n",
        "        ########################################################################\n",
        ""
      ],
      "metadata": {
        "id": "EeDRPdEZTrMf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 128\n",
        "lr = 0.01\n",
        "lr_decay_factor = 0.5\n",
        "lr_decay_step_size = 50\n",
        "\n",
        "layers = [5]\n",
        "hiddens = [64]\n",
        "datasets = ['IMDB-BINARY']\n",
        "nets = [\n",
        "    GIN0,\n",
        "    GIN,\n",
        "]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def logger(info):\n",
        "    fold, epoch = info['fold'] + 1, info['epoch']\n",
        "    val_loss, test_acc = info['val_loss'], info['test_acc']\n",
        "    print('{:02d}/{:03d}: Val Loss: {:.4f}, Test Accuracy: {:.3f}'.format(\n",
        "        fold, epoch, val_loss, test_acc))\n",
        "\n",
        "\n",
        "results = []\n",
        "for dataset_name, Net in product(datasets, nets):\n",
        "    best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
        "    print('-----\\n{} - {}'.format(dataset_name, Net.__name__))\n",
        "    for num_layers, hidden in product(layers, hiddens):\n",
        "        dataset = get_dataset(dataset_name)\n",
        "        model = Net(dataset, num_layers, hidden)\n",
        "        loss, acc, std = cross_validation_with_val_set(\n",
        "            dataset,\n",
        "            model,\n",
        "            folds=10,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            lr_decay_factor=lr_decay_factor,\n",
        "            lr_decay_step_size=lr_decay_step_size,\n",
        "            weight_decay=0,\n",
        "            logger=None,\n",
        "        )\n",
        "        if loss < best_result[0]:\n",
        "            best_result = (loss, acc, std)\n",
        "\n",
        "    desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
        "    print('Best result - {}'.format(desc))\n",
        "    results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
        "print('-----\\n{}'.format('\\n'.join(results)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DMrGcgDTvV6",
        "outputId": "8d517967-c3e1-4a7c-a956-841781a5a6ed"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "IMDB-BINARY - GIN0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.4663, Test Accuracy: 0.718 ± 0.027, Duration: 16.681\n",
            "Best result - 0.718 ± 0.027\n",
            "-----\n",
            "IMDB-BINARY - GIN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.4621, Test Accuracy: 0.728 ± 0.056, Duration: 16.301\n",
            "Best result - 0.728 ± 0.056\n",
            "-----\n",
            "IMDB-BINARY - GIN0(\n",
            "  (conv1): GINConv()\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x GINConv()\n",
            "  )\n",
            "  (lin1): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (lin2): Linear(in_features=64, out_features=2, bias=True)\n",
            "): 0.718 ± 0.027\n",
            "IMDB-BINARY - GIN(\n",
            "  (conv1): GINConv()\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x GINConv()\n",
            "  )\n",
            "  (lin1): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (lin2): Linear(in_features=64, out_features=2, bias=True)\n",
            "): 0.728 ± 0.056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 128\n",
        "lr = 0.01\n",
        "lr_decay_factor = 0.5\n",
        "lr_decay_step_size = 50\n",
        "\n",
        "layers = [5]\n",
        "hiddens = [64]\n",
        "datasets = ['IMDB-BINARY']\n",
        "nets = [\n",
        "    GIN0,\n",
        "    GIN,\n",
        "]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def logger(info):\n",
        "    fold, epoch = info['fold'] + 1, info['epoch']\n",
        "    val_loss, test_acc = info['val_loss'], info['test_acc']\n",
        "    print('{:02d}/{:03d}: Val Loss: {:.4f}, Test Accuracy: {:.3f}'.format(\n",
        "        fold, epoch, val_loss, test_acc))\n",
        "\n",
        "\n",
        "results = []\n",
        "for dataset_name, Net in product(datasets, nets):\n",
        "    best_result = (float('inf'), 0, 0)  # (loss, acc, std)\n",
        "    print('-----\\n{} - {}'.format(dataset_name, Net.__name__))\n",
        "    for num_layers, hidden in product(layers, hiddens):\n",
        "        dataset = get_dataset(dataset_name)\n",
        "        model = Net(dataset, num_layers, hidden)\n",
        "        loss, acc, std = cross_validation_with_val_set(\n",
        "            dataset,\n",
        "            model,\n",
        "            folds=10,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            lr_decay_factor=lr_decay_factor,\n",
        "            lr_decay_step_size=lr_decay_step_size,\n",
        "            weight_decay=0,\n",
        "            logger=None,\n",
        "        )\n",
        "        if loss < best_result[0]:\n",
        "            best_result = (loss, acc, std)\n",
        "\n",
        "    desc = '{:.3f} ± {:.3f}'.format(best_result[1], best_result[2])\n",
        "    print('Best result - {}'.format(desc))\n",
        "    results += ['{} - {}: {}'.format(dataset_name, model, desc)]\n",
        "print('-----\\n{}'.format('\\n'.join(results)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE2vktV1Tymw",
        "outputId": "4039313c-bcb8-4822-8018-3a37c7ed8b53"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "IMDB-BINARY - GIN0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.4617, Test Accuracy: 0.738 ± 0.056, Duration: 16.094\n",
            "Best result - 0.738 ± 0.056\n",
            "-----\n",
            "IMDB-BINARY - GIN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.4594, Test Accuracy: 0.711 ± 0.035, Duration: 16.397\n",
            "Best result - 0.711 ± 0.035\n",
            "-----\n",
            "IMDB-BINARY - GIN0(\n",
            "  (conv1): GINConv()\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x GINConv()\n",
            "  )\n",
            "  (lin1): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (lin2): Linear(in_features=64, out_features=2, bias=True)\n",
            "): 0.738 ± 0.056\n",
            "IMDB-BINARY - GIN(\n",
            "  (conv1): GINConv()\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x GINConv()\n",
            "  )\n",
            "  (lin1): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (lin2): Linear(in_features=64, out_features=2, bias=True)\n",
            "): 0.711 ± 0.035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LBXASFVZT1jh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}